{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create simple generators for Machine Learning and Deep Learning\n",
    "\n",
    "This tutorial explain how to **write differents iterators** and what **features** these iterators can implement for machine learning tasks. It also show the usage of **`machinelearning.iterator` tools**.\n",
    "\n",
    "Usefull for case like **deep learning train (e.g. on Keras with `fit_generator`)**. The tool will help you to easily create **generators which yield batches of samples infinitely in multiple processes**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A fake dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a fake dataset which will contains **documents** and **labels** with this structure:\n",
    "    \n",
    "    A this is a document\n",
    "    B an other document\n",
    "    A this talk about things\n",
    "    C the apple is green or yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['part0.txt', 'part1.txt', 'part2.txt']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "voc = \"this is a an other document talk about things the apple is green or yellow\".split()\n",
    "paths = [\"part\" + str(i) + \".txt\" for i in range(3)]\n",
    "print(paths)\n",
    "for path in paths:\n",
    "    with open(path, \"w\") as f:\n",
    "        for sampleId in range(2):\n",
    "            line = \"\"\n",
    "            # We first add the class of the document:\n",
    "            line = random.choice(['A', 'B', 'C']) + \" \"\n",
    "            # Then we add a random words:\n",
    "            for i in range(5):\n",
    "                line += random.choice(voc) + \" \"\n",
    "            f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generators and iterators are usefull in cases **you cannot load your dataset into memory**. Lets started with a **simple generator** which takes a container (a file) in parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataGenerator(container, *args, **kwargs):\n",
    "    with open(container) as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(\" \")\n",
    "            yield (line[1:], line[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And iterate over the whole dataset (single-processing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B --> ['about', 'is', 'about', 'the', 'talk']\n",
      "A --> ['document', 'about', 'document', 'things', 'the']\n",
      "A --> ['things', 'is', 'green', 'this', 'or']\n",
      "A --> ['green', 'is', 'the', 'a', 'things']\n",
      "B --> ['yellow', 'apple', 'other', 'things', 'other']\n",
      "A --> ['talk', 'a', 'a', 'apple', 'other']\n"
     ]
    }
   ],
   "source": [
    "for path in paths:\n",
    "    for tokens, label in dataGenerator(path):\n",
    "        print(label + \" --> \" + str(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A multi-processing iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in case you **compressed files** and your **disk storage is sufficiently fast**, or in cases the preprocessing of data is **time consuming**, it is beneficial to read files in a multi-processing way so files will be uncompressed on **multiple cores**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this you can simply use `machinelearning.iterator.ConsistentIterator` which takes a **list of containers** (typically a list of files) and a function which is a **generator function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machinelearning import iterator as mlit\n",
    "ci = mlit.ConsistentIterator(paths, dataGenerator, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And iterate over the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B --> ['about', 'is', 'about', 'the', 'talk']\n",
      "A --> ['things', 'is', 'green', 'this', 'or']\n",
      "B --> ['yellow', 'apple', 'other', 'things', 'other']\n",
      "A --> ['document', 'about', 'document', 'things', 'the']\n",
      "A --> ['green', 'is', 'the', 'a', 'things']\n",
      "A --> ['talk', 'a', 'a', 'apple', 'other']\n"
     ]
    }
   ],
   "source": [
    "for tokens, label in ci:\n",
    "    print(label + \" --> \" + str(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features:**\n",
    "\n",
    " * Multi-processing\n",
    " * Always generate data in the same order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make it multi-iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main problem of this iterator is that if you give an instance of your iterator to an external tool like [`Doc2Vec` from Gensim](https://radimrehurek.com/gensim/models/doc2vec.html), the tool **won't be able to iterate several time over your dataset**. Demonstration, you try to iterate again the instance `ci`, you will directly leave the loop: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "print(\"Start\")\n",
    "for _ in ci: print(\"Got one\")\n",
    "print(\"End\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is to **wrap your iterator initialization** in `AgainAndAgain` which takes a generator (or an Iterator class like `ConsistentIterator`) and all parameters to propagate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaaCI = mlit.AgainAndAgain(mlit.ConsistentIterator, paths, dataGenerator, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Got one\n",
      "Got one\n",
      "Got one\n",
      "Got one\n",
      "Got one\n",
      "Got one\n",
      "Got one again\n",
      "Got one again\n",
      "Got one again\n",
      "Got one again\n",
      "Got one again\n",
      "Got one again\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "print(\"Start\")\n",
    "for _ in aaaCI: print(\"Got one\")\n",
    "for _ in aaaCI: print(\"Got one again\")\n",
    "print(\"End\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features:**\n",
    "\n",
    " * Multi-iterable (the iterator instance embbed init parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate infinite batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some libraries take generators which must **yield batches of samples**. For instance, **[`fit_generator`](https://keras.io/models/sequential/#fit_generator) from Keras** takes a generator which must generate data this way (here with a batch size of 2):\n",
    "\n",
    "    ([doc1, doc2], [label1, label2])\n",
    "    ([doc3, doc4], [label3, label4])\n",
    "    ([doc5, doc6], [label5, label6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just need to **wrap an AgainAndAgain instance**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "infiniteBatches = mlit.InfiniteBatcher(aaaCI, batchSize=2, toNumpyArray=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And Keras will iterate over your dataset this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['about', 'is', 'about', 'the', 'talk'], ['things', 'is', 'green', 'this', 'or']], ['B', 'A'])\n",
      "([['yellow', 'apple', 'other', 'things', 'other'], ['document', 'about', 'document', 'things', 'the']], ['B', 'A'])\n",
      "([['green', 'is', 'the', 'a', 'things'], ['talk', 'a', 'a', 'apple', 'other']], ['A', 'A'])\n",
      "([['about', 'is', 'about', 'the', 'talk'], ['things', 'is', 'green', 'this', 'or']], ['B', 'A'])\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(next(infiniteBatches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **[`fit_generator`](https://keras.io/models/sequential/#fit_generator) from Keras**, you need to give the InfiniteBatcher instance, specify the **number of steps** (`steps_per_epoch`) to terminate an epoch, and specify **the number of epochs**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use these **optionnal parameters**:\n",
    "\n",
    " * `shuffle` (integer) which will indicate how many batches to aggregate and shuffle\n",
    " * `skip` (integer) to skip some samples at the beggining in cases you resume a train from a previous run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features:**\n",
    "\n",
    " * Generate batches\n",
    " * Infinite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use variables in the data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you want to **remove stop words** in multiple processes while you iterate your dataset. You just need to use `subProcessParseFunct` and `subProcessParseFunctKwargs`. This function will take one input from your base generator and must returned the processed item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define stop words and a function which remoave stop word of a single data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = [\"a\", \"an\", \"the\", \"or\", \"the\", \"this\", \"is\"]\n",
    "def removeStopWords(data, *args, stopWords=set(), **kwargs):\n",
    "    tokens, label = data\n",
    "    tokens = [word for word in tokens if word not in stopWords]\n",
    "    return (tokens, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we init a `ConsistentIterator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = mlit.ConsistentIterator(paths, dataGenerator, verbose=False,\n",
    "                             subProcessParseFunct=removeStopWords,\n",
    "                             subProcessParseFunctKwargs={\"stopWords\": stopWords})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally iterate the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B --> ['about', 'about', 'talk']\n",
      "A --> ['things', 'green']\n",
      "B --> ['yellow', 'apple', 'other', 'things', 'other']\n",
      "A --> ['document', 'about', 'document', 'things']\n",
      "A --> ['green', 'things']\n",
      "A --> ['talk', 'apple', 'other']\n"
     ]
    }
   ],
   "source": [
    "for tokens, label in ci:\n",
    "    print(label + \" --> \" + str(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, like we saw, you can wrap it in `AgainAndAgain` and an `InfiniteBatcher`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that **you cannot share variables across processes** (python serialize each variables so it will be differents instances, differents copies), in this example, **`stopWords` will be replicated** in each process. If you want to set global variables on-the-fly during the iteration, use `mainProcessParseFunct` and `mainProcessParseFunctKwargs`. This function takes one output from previous function and must return the processed item (single-processing). You can also give parameters for the base generator using `itemGeneratorKwargs`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
